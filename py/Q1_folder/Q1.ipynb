{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree classifier for SDSS DR14\n",
    "\n",
    "This data set is taken from the fourteenth data release of the Sloan Digital Sky Survey (SDSS DR14). It consists of 10,000 records of observations from SDSS. Each observation is then identified as either a star, galaxy or a quasar.\n",
    "\n",
    "In this notebook we will explore how we can code a decision tree to classify each celestial obejct within this data set (stars, galaxies and quasars), as well as looking at the overall accuracy of this classical machine learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Libraries\n",
    "\n",
    "Here we will install the necessary libraries that we need to make the decision tree classifier. We will be using pandas, Scikit-learn and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# Used to read the data files that are uploaded to the notebook\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier \u001b[38;5;66;03m# Used to create and train a decision tree for classification tasks\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split \u001b[38;5;66;03m# Splits the data into training and testing subsets\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Used to read the data files that are uploaded to the notebook\n",
    "from sklearn.tree import DecisionTreeClassifier # Used to create and train a decision tree for classification tasks\n",
    "from sklearn.model_selection import train_test_split # Splits the data into training and testing subsets\n",
    "from sklearn.tree import plot_tree # Used to visualise a trained tree in a graphical format\n",
    "from sklearn.metrics import classification_report, accuracy_score # This will generate a detailed report on classification performance. accuracy score will compute the overall accuracy of the preidictions\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # Generates a confusion matrix to show true or false positives or negatives, as well as visualising the matrix with a heatmap\n",
    "import matplotlib.pyplot as plt # Data visualisation library for Python, needed to make the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and understanding the data set\n",
    "In order to analyse the data, we will first have to load the data set and read it with pandas.\n",
    "\n",
    "We will then preview what is within this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Skyserver_SQL2_27_2018 6_51_39 PM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll print out the first few rows of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see from this table is the observed object's ID, the objects position in the sky, right ascention (ra) and declination (dec). The magnitudes observed in each of the telescope's filters (u, g, r, i and z), the class of each object and their redshifts. Note how some of the redshifts are negative, this means that this is a blueshift (the object is moving towards us)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features and target variables\n",
    "Now that we understand what the dataset describes, we can now move on to selecting our featres that we want to include in our model from the data, as well as chosing what we want our model to target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['u', 'g', 'r', 'i', 'z']]  # Optionally include redshift for higher accuracy\n",
    "target = data['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are focusing on classifying the celestial objects so we choose our target to be the 'class' of each object, but the target can be changed to any columm from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data and training the model\n",
    "In this section, we will split the data intro training and testing subsets.\n",
    "\n",
    "We split up the data into these subsets so that we can allow the model to learn patterns, rules and relationships from the dataset. Then the testing subset is then used to evaluate our model's perfomance on unseen data.\n",
    "\n",
    "Typically we split the data into a 80:20 ratio, this means we train the data with 80% of the data and then evaluate the performance of the model with the remaining 20%\n",
    "\n",
    "Splitting the data up also allows us to avoid overfitting the model. Essentially this means that the model will memorise the training data, which gives a strong performance on that data but a very poor performance on the unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio between the training and testing subsets can be modified freely, try playing with the ratio and see how the model performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling the tree's complexity\n",
    "Now that the data is split into the training and testing subsets, we can tweak how complex or 'deep' we want our tree to be. This is depth control and it will limit how the model captures patterns in the data.\n",
    "\n",
    "A deeper tree can capture more patterns in the data, but this can lead to overfitting, however if the depth is set too low then the model will begin to underfit the data.\n",
    "\n",
    "Controlling the depth is also linked to the bias and variance of the model. A deeper tree will lead to less bias but more variance, whereas a shallower tree will generalise the data better, but could potentially be more biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth = 4, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll set the max_depth to be 4 so that later on we can better visualise the process behind the tree, but this can be adjusted if the model begins to underfit.\n",
    "\n",
    "We also use the random state variable, this is to control the randomness of certain functions in the machine learning library such as `train_test_split` or initialising random processes, e.g. `DecisionTreeClassifier`. We set this variable to 42 as this ensures consistent results across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "Having played around with the training and testing ratio as well as implementing some depth control to our model, we can now evaluate how our model performs when we introduce it to the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test) # This is where we make our predictions with the model\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) # This will compute the accuracy of the model\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred) # This will generate a detailed report on classification performance\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation\n",
    "Here we can visualise the tree so that we can better understand the processes behind the tree. Visualising the tree allows us to diagnose if the model is underperforming or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf, feature_names=features.columns, class_names=clf.classes_, filled=True, fontsize=10)\n",
    "plt.title(\"Decision Tree Visualisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a confusion matrix, so that we can see the model performance and behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=clf.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
